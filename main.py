import requests
from bs4 import BeautifulSoup
from urllib.parse import urlencode
from tqdm import tqdm
import time
import json

# æ‰€æœ‰å¯èƒ½çš„ç¨€æœ‰åº¦ä»£ç¢¼èˆ‡åç¨±å°æ‡‰
RARITY_FULL_MAP = {
    1: "C",
    2: "U",
    3: "R",
    4: "RR",
    5: "RRR",
    6: "PR",
    7: "TR",
    8: "SR",
    9: "HR",
    10: "UR",
    11: "ç„¡æ¨™è¨˜",
    12: "K",
    13: "A",
    14: "AR",
    15: "SAR",
    16: "S",
    17: "SSR",
    18: "ACE",
}

# é€šç”¨æŸ¥è©¢åƒæ•¸
COMMON_PARAMS = {
    "sortCondition": "",
    "keyword": "",
    "cardType": "all",
    "regulation": "1",
    "pokemonEnergy": "",
    "pokemonWeakness": "",
    "pokemonResistance": "",
    "pokemonMoveEnergy": "",
    "hpLowerLimit": "none",
    "hpUpperLimit": "none",
    "retreatCostLowerLimit": "0",
    "retreatCostUpperLimit": "none",
    "illustratorName": "",
    "expansionCodes": "",
}

# ç›®æ¨™ç¶²ç«™
BASE_URL = "https://asia.pokemon-card.com/tw/card-search/list/"


# æŠ“å–å–®ä¸€ç¨€æœ‰åº¦å¡ç‰‡
def scrape_cards_by_rarity(rarity_code, rarity_name):
    print(f"\nğŸ” æ­£åœ¨æŠ“å–ã€Œ{rarity_name}ã€...")

    card_images = []

    params = COMMON_PARAMS.copy()
    params["rarity[]"] = rarity_code
    params_encoded = urlencode(params, doseq=True)

    first_page = requests.get(f"{BASE_URL}?{params_encoded}")
    soup = BeautifulSoup(first_page.text, "html.parser")

    try:
        page_text = soup.select_one("p.resultTotalPages").text.strip()
        total_pages = int(page_text.replace("/ å…±", "").replace("é ", "").strip())
    except Exception:
        total_pages = 1

    for page in tqdm(range(1, total_pages + 1), desc=f"{rarity_name} é é¢"):
        params["pageNo"] = page
        page_html = requests.get(f"{BASE_URL}?{urlencode(params, doseq=True)}")
        soup = BeautifulSoup(page_html.text, "html.parser")

        for img in soup.select("img.lazy"):
            url = img.get("data-original")
            if url:
                card_images.append(url)

        time.sleep(0.5)

    return card_images


# åŸ·è¡Œä¸¦åˆ†é¡
all_cards = {}

for rarity_code, rarity_name in RARITY_FULL_MAP.items():
    images = scrape_cards_by_rarity(rarity_code, rarity_name)
    if images:
        all_cards[rarity_name] = images

# é¡¯ç¤ºç¸½çµ
print("\nğŸ“¦ å…¨éƒ¨æŠ“å–å®Œç•¢ï¼çµæœå¦‚ä¸‹ï¼š")
for rarity, urls in all_cards.items():
    print(f"\n{rarity}ï¼ˆ{len(urls)} å¼µï¼‰ï¼š")
    for u in urls:
        print(f"- {u}")

# è¼¸å‡º JSON
with open("pokemon_cards.json", "w", encoding="utf-8") as f:
    json.dump(all_cards, f, indent=2, ensure_ascii=False)

print("\nâœ… å·²è¼¸å‡ºç‚º pokemon_cards.json")
